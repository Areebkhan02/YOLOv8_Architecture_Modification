{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/areebadnan/Areeb_code/work/Atheritia\n",
      "/home/areebadnan/Areeb_code/work/Atheritia/ultralytics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/areebadnan/Areeb_Python_Environments/yolo_env1/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%pwd\n",
    "%cd ..\n",
    "%cd ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/areebadnan/Areeb_code/work/Atheritia/ultralytics/ultralytics/nn/tasks.py:785: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(file, map_location=\"cpu\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "odict_keys(['model.model.0.conv.weight', 'model.model.0.bn.weight', 'model.model.0.bn.bias', 'model.model.0.bn.running_mean', 'model.model.0.bn.running_var', 'model.model.0.bn.num_batches_tracked', 'model.model.1.conv.weight', 'model.model.1.bn.weight', 'model.model.1.bn.bias', 'model.model.1.bn.running_mean', 'model.model.1.bn.running_var', 'model.model.1.bn.num_batches_tracked', 'model.model.2.cv1.conv.weight', 'model.model.2.cv1.bn.weight', 'model.model.2.cv1.bn.bias', 'model.model.2.cv1.bn.running_mean', 'model.model.2.cv1.bn.running_var', 'model.model.2.cv1.bn.num_batches_tracked', 'model.model.2.cv2.conv.weight', 'model.model.2.cv2.bn.weight', 'model.model.2.cv2.bn.bias', 'model.model.2.cv2.bn.running_mean', 'model.model.2.cv2.bn.running_var', 'model.model.2.cv2.bn.num_batches_tracked', 'model.model.2.m.0.cv1.conv.weight', 'model.model.2.m.0.cv1.bn.weight', 'model.model.2.m.0.cv1.bn.bias', 'model.model.2.m.0.cv1.bn.running_mean', 'model.model.2.m.0.cv1.bn.running_var', 'model.model.2.m.0.cv1.bn.num_batches_tracked', 'model.model.2.m.0.cv2.conv.weight', 'model.model.2.m.0.cv2.bn.weight', 'model.model.2.m.0.cv2.bn.bias', 'model.model.2.m.0.cv2.bn.running_mean', 'model.model.2.m.0.cv2.bn.running_var', 'model.model.2.m.0.cv2.bn.num_batches_tracked', 'model.model.2.m.1.cv1.conv.weight', 'model.model.2.m.1.cv1.bn.weight', 'model.model.2.m.1.cv1.bn.bias', 'model.model.2.m.1.cv1.bn.running_mean', 'model.model.2.m.1.cv1.bn.running_var', 'model.model.2.m.1.cv1.bn.num_batches_tracked', 'model.model.2.m.1.cv2.conv.weight', 'model.model.2.m.1.cv2.bn.weight', 'model.model.2.m.1.cv2.bn.bias', 'model.model.2.m.1.cv2.bn.running_mean', 'model.model.2.m.1.cv2.bn.running_var', 'model.model.2.m.1.cv2.bn.num_batches_tracked', 'model.model.2.m.2.cv1.conv.weight', 'model.model.2.m.2.cv1.bn.weight', 'model.model.2.m.2.cv1.bn.bias', 'model.model.2.m.2.cv1.bn.running_mean', 'model.model.2.m.2.cv1.bn.running_var', 'model.model.2.m.2.cv1.bn.num_batches_tracked', 'model.model.2.m.2.cv2.conv.weight', 'model.model.2.m.2.cv2.bn.weight', 'model.model.2.m.2.cv2.bn.bias', 'model.model.2.m.2.cv2.bn.running_mean', 'model.model.2.m.2.cv2.bn.running_var', 'model.model.2.m.2.cv2.bn.num_batches_tracked', 'model.model.3.conv.weight', 'model.model.3.bn.weight', 'model.model.3.bn.bias', 'model.model.3.bn.running_mean', 'model.model.3.bn.running_var', 'model.model.3.bn.num_batches_tracked', 'model.model.4.cv1.conv.weight', 'model.model.4.cv1.bn.weight', 'model.model.4.cv1.bn.bias', 'model.model.4.cv1.bn.running_mean', 'model.model.4.cv1.bn.running_var', 'model.model.4.cv1.bn.num_batches_tracked', 'model.model.4.cv2.conv.weight', 'model.model.4.cv2.bn.weight', 'model.model.4.cv2.bn.bias', 'model.model.4.cv2.bn.running_mean', 'model.model.4.cv2.bn.running_var', 'model.model.4.cv2.bn.num_batches_tracked', 'model.model.4.m.0.cv1.conv.weight', 'model.model.4.m.0.cv1.bn.weight', 'model.model.4.m.0.cv1.bn.bias', 'model.model.4.m.0.cv1.bn.running_mean', 'model.model.4.m.0.cv1.bn.running_var', 'model.model.4.m.0.cv1.bn.num_batches_tracked', 'model.model.4.m.0.cv2.conv.weight', 'model.model.4.m.0.cv2.bn.weight', 'model.model.4.m.0.cv2.bn.bias', 'model.model.4.m.0.cv2.bn.running_mean', 'model.model.4.m.0.cv2.bn.running_var', 'model.model.4.m.0.cv2.bn.num_batches_tracked', 'model.model.4.m.1.cv1.conv.weight', 'model.model.4.m.1.cv1.bn.weight', 'model.model.4.m.1.cv1.bn.bias', 'model.model.4.m.1.cv1.bn.running_mean', 'model.model.4.m.1.cv1.bn.running_var', 'model.model.4.m.1.cv1.bn.num_batches_tracked', 'model.model.4.m.1.cv2.conv.weight', 'model.model.4.m.1.cv2.bn.weight', 'model.model.4.m.1.cv2.bn.bias', 'model.model.4.m.1.cv2.bn.running_mean', 'model.model.4.m.1.cv2.bn.running_var', 'model.model.4.m.1.cv2.bn.num_batches_tracked', 'model.model.4.m.2.cv1.conv.weight', 'model.model.4.m.2.cv1.bn.weight', 'model.model.4.m.2.cv1.bn.bias', 'model.model.4.m.2.cv1.bn.running_mean', 'model.model.4.m.2.cv1.bn.running_var', 'model.model.4.m.2.cv1.bn.num_batches_tracked', 'model.model.4.m.2.cv2.conv.weight', 'model.model.4.m.2.cv2.bn.weight', 'model.model.4.m.2.cv2.bn.bias', 'model.model.4.m.2.cv2.bn.running_mean', 'model.model.4.m.2.cv2.bn.running_var', 'model.model.4.m.2.cv2.bn.num_batches_tracked', 'model.model.4.m.3.cv1.conv.weight', 'model.model.4.m.3.cv1.bn.weight', 'model.model.4.m.3.cv1.bn.bias', 'model.model.4.m.3.cv1.bn.running_mean', 'model.model.4.m.3.cv1.bn.running_var', 'model.model.4.m.3.cv1.bn.num_batches_tracked', 'model.model.4.m.3.cv2.conv.weight', 'model.model.4.m.3.cv2.bn.weight', 'model.model.4.m.3.cv2.bn.bias', 'model.model.4.m.3.cv2.bn.running_mean', 'model.model.4.m.3.cv2.bn.running_var', 'model.model.4.m.3.cv2.bn.num_batches_tracked', 'model.model.4.m.4.cv1.conv.weight', 'model.model.4.m.4.cv1.bn.weight', 'model.model.4.m.4.cv1.bn.bias', 'model.model.4.m.4.cv1.bn.running_mean', 'model.model.4.m.4.cv1.bn.running_var', 'model.model.4.m.4.cv1.bn.num_batches_tracked', 'model.model.4.m.4.cv2.conv.weight', 'model.model.4.m.4.cv2.bn.weight', 'model.model.4.m.4.cv2.bn.bias', 'model.model.4.m.4.cv2.bn.running_mean', 'model.model.4.m.4.cv2.bn.running_var', 'model.model.4.m.4.cv2.bn.num_batches_tracked', 'model.model.4.m.5.cv1.conv.weight', 'model.model.4.m.5.cv1.bn.weight', 'model.model.4.m.5.cv1.bn.bias', 'model.model.4.m.5.cv1.bn.running_mean', 'model.model.4.m.5.cv1.bn.running_var', 'model.model.4.m.5.cv1.bn.num_batches_tracked', 'model.model.4.m.5.cv2.conv.weight', 'model.model.4.m.5.cv2.bn.weight', 'model.model.4.m.5.cv2.bn.bias', 'model.model.4.m.5.cv2.bn.running_mean', 'model.model.4.m.5.cv2.bn.running_var', 'model.model.4.m.5.cv2.bn.num_batches_tracked', 'model.model.5.conv.weight', 'model.model.5.bn.weight', 'model.model.5.bn.bias', 'model.model.5.bn.running_mean', 'model.model.5.bn.running_var', 'model.model.5.bn.num_batches_tracked', 'model.model.6.cv1.conv.weight', 'model.model.6.cv1.bn.weight', 'model.model.6.cv1.bn.bias', 'model.model.6.cv1.bn.running_mean', 'model.model.6.cv1.bn.running_var', 'model.model.6.cv1.bn.num_batches_tracked', 'model.model.6.cv2.conv.weight', 'model.model.6.cv2.bn.weight', 'model.model.6.cv2.bn.bias', 'model.model.6.cv2.bn.running_mean', 'model.model.6.cv2.bn.running_var', 'model.model.6.cv2.bn.num_batches_tracked', 'model.model.6.m.0.cv1.conv.weight', 'model.model.6.m.0.cv1.bn.weight', 'model.model.6.m.0.cv1.bn.bias', 'model.model.6.m.0.cv1.bn.running_mean', 'model.model.6.m.0.cv1.bn.running_var', 'model.model.6.m.0.cv1.bn.num_batches_tracked', 'model.model.6.m.0.cv2.conv.weight', 'model.model.6.m.0.cv2.bn.weight', 'model.model.6.m.0.cv2.bn.bias', 'model.model.6.m.0.cv2.bn.running_mean', 'model.model.6.m.0.cv2.bn.running_var', 'model.model.6.m.0.cv2.bn.num_batches_tracked', 'model.model.6.m.1.cv1.conv.weight', 'model.model.6.m.1.cv1.bn.weight', 'model.model.6.m.1.cv1.bn.bias', 'model.model.6.m.1.cv1.bn.running_mean', 'model.model.6.m.1.cv1.bn.running_var', 'model.model.6.m.1.cv1.bn.num_batches_tracked', 'model.model.6.m.1.cv2.conv.weight', 'model.model.6.m.1.cv2.bn.weight', 'model.model.6.m.1.cv2.bn.bias', 'model.model.6.m.1.cv2.bn.running_mean', 'model.model.6.m.1.cv2.bn.running_var', 'model.model.6.m.1.cv2.bn.num_batches_tracked', 'model.model.6.m.2.cv1.conv.weight', 'model.model.6.m.2.cv1.bn.weight', 'model.model.6.m.2.cv1.bn.bias', 'model.model.6.m.2.cv1.bn.running_mean', 'model.model.6.m.2.cv1.bn.running_var', 'model.model.6.m.2.cv1.bn.num_batches_tracked', 'model.model.6.m.2.cv2.conv.weight', 'model.model.6.m.2.cv2.bn.weight', 'model.model.6.m.2.cv2.bn.bias', 'model.model.6.m.2.cv2.bn.running_mean', 'model.model.6.m.2.cv2.bn.running_var', 'model.model.6.m.2.cv2.bn.num_batches_tracked', 'model.model.6.m.3.cv1.conv.weight', 'model.model.6.m.3.cv1.bn.weight', 'model.model.6.m.3.cv1.bn.bias', 'model.model.6.m.3.cv1.bn.running_mean', 'model.model.6.m.3.cv1.bn.running_var', 'model.model.6.m.3.cv1.bn.num_batches_tracked', 'model.model.6.m.3.cv2.conv.weight', 'model.model.6.m.3.cv2.bn.weight', 'model.model.6.m.3.cv2.bn.bias', 'model.model.6.m.3.cv2.bn.running_mean', 'model.model.6.m.3.cv2.bn.running_var', 'model.model.6.m.3.cv2.bn.num_batches_tracked', 'model.model.6.m.4.cv1.conv.weight', 'model.model.6.m.4.cv1.bn.weight', 'model.model.6.m.4.cv1.bn.bias', 'model.model.6.m.4.cv1.bn.running_mean', 'model.model.6.m.4.cv1.bn.running_var', 'model.model.6.m.4.cv1.bn.num_batches_tracked', 'model.model.6.m.4.cv2.conv.weight', 'model.model.6.m.4.cv2.bn.weight', 'model.model.6.m.4.cv2.bn.bias', 'model.model.6.m.4.cv2.bn.running_mean', 'model.model.6.m.4.cv2.bn.running_var', 'model.model.6.m.4.cv2.bn.num_batches_tracked', 'model.model.6.m.5.cv1.conv.weight', 'model.model.6.m.5.cv1.bn.weight', 'model.model.6.m.5.cv1.bn.bias', 'model.model.6.m.5.cv1.bn.running_mean', 'model.model.6.m.5.cv1.bn.running_var', 'model.model.6.m.5.cv1.bn.num_batches_tracked', 'model.model.6.m.5.cv2.conv.weight', 'model.model.6.m.5.cv2.bn.weight', 'model.model.6.m.5.cv2.bn.bias', 'model.model.6.m.5.cv2.bn.running_mean', 'model.model.6.m.5.cv2.bn.running_var', 'model.model.6.m.5.cv2.bn.num_batches_tracked', 'model.model.7.conv.weight', 'model.model.7.bn.weight', 'model.model.7.bn.bias', 'model.model.7.bn.running_mean', 'model.model.7.bn.running_var', 'model.model.7.bn.num_batches_tracked', 'model.model.8.cv1.conv.weight', 'model.model.8.cv1.bn.weight', 'model.model.8.cv1.bn.bias', 'model.model.8.cv1.bn.running_mean', 'model.model.8.cv1.bn.running_var', 'model.model.8.cv1.bn.num_batches_tracked', 'model.model.8.cv2.conv.weight', 'model.model.8.cv2.bn.weight', 'model.model.8.cv2.bn.bias', 'model.model.8.cv2.bn.running_mean', 'model.model.8.cv2.bn.running_var', 'model.model.8.cv2.bn.num_batches_tracked', 'model.model.8.m.0.cv1.conv.weight', 'model.model.8.m.0.cv1.bn.weight', 'model.model.8.m.0.cv1.bn.bias', 'model.model.8.m.0.cv1.bn.running_mean', 'model.model.8.m.0.cv1.bn.running_var', 'model.model.8.m.0.cv1.bn.num_batches_tracked', 'model.model.8.m.0.cv2.conv.weight', 'model.model.8.m.0.cv2.bn.weight', 'model.model.8.m.0.cv2.bn.bias', 'model.model.8.m.0.cv2.bn.running_mean', 'model.model.8.m.0.cv2.bn.running_var', 'model.model.8.m.0.cv2.bn.num_batches_tracked', 'model.model.8.m.1.cv1.conv.weight', 'model.model.8.m.1.cv1.bn.weight', 'model.model.8.m.1.cv1.bn.bias', 'model.model.8.m.1.cv1.bn.running_mean', 'model.model.8.m.1.cv1.bn.running_var', 'model.model.8.m.1.cv1.bn.num_batches_tracked', 'model.model.8.m.1.cv2.conv.weight', 'model.model.8.m.1.cv2.bn.weight', 'model.model.8.m.1.cv2.bn.bias', 'model.model.8.m.1.cv2.bn.running_mean', 'model.model.8.m.1.cv2.bn.running_var', 'model.model.8.m.1.cv2.bn.num_batches_tracked', 'model.model.8.m.2.cv1.conv.weight', 'model.model.8.m.2.cv1.bn.weight', 'model.model.8.m.2.cv1.bn.bias', 'model.model.8.m.2.cv1.bn.running_mean', 'model.model.8.m.2.cv1.bn.running_var', 'model.model.8.m.2.cv1.bn.num_batches_tracked', 'model.model.8.m.2.cv2.conv.weight', 'model.model.8.m.2.cv2.bn.weight', 'model.model.8.m.2.cv2.bn.bias', 'model.model.8.m.2.cv2.bn.running_mean', 'model.model.8.m.2.cv2.bn.running_var', 'model.model.8.m.2.cv2.bn.num_batches_tracked', 'model.model.9.cv1.conv.weight', 'model.model.9.cv1.bn.weight', 'model.model.9.cv1.bn.bias', 'model.model.9.cv1.bn.running_mean', 'model.model.9.cv1.bn.running_var', 'model.model.9.cv1.bn.num_batches_tracked', 'model.model.9.cv2.conv.weight', 'model.model.9.cv2.bn.weight', 'model.model.9.cv2.bn.bias', 'model.model.9.cv2.bn.running_mean', 'model.model.9.cv2.bn.running_var', 'model.model.9.cv2.bn.num_batches_tracked', 'model.model.12.cv1.conv.weight', 'model.model.12.cv1.bn.weight', 'model.model.12.cv1.bn.bias', 'model.model.12.cv1.bn.running_mean', 'model.model.12.cv1.bn.running_var', 'model.model.12.cv1.bn.num_batches_tracked', 'model.model.12.cv2.conv.weight', 'model.model.12.cv2.bn.weight', 'model.model.12.cv2.bn.bias', 'model.model.12.cv2.bn.running_mean', 'model.model.12.cv2.bn.running_var', 'model.model.12.cv2.bn.num_batches_tracked', 'model.model.12.m.0.cv1.conv.weight', 'model.model.12.m.0.cv1.bn.weight', 'model.model.12.m.0.cv1.bn.bias', 'model.model.12.m.0.cv1.bn.running_mean', 'model.model.12.m.0.cv1.bn.running_var', 'model.model.12.m.0.cv1.bn.num_batches_tracked', 'model.model.12.m.0.cv2.conv.weight', 'model.model.12.m.0.cv2.bn.weight', 'model.model.12.m.0.cv2.bn.bias', 'model.model.12.m.0.cv2.bn.running_mean', 'model.model.12.m.0.cv2.bn.running_var', 'model.model.12.m.0.cv2.bn.num_batches_tracked', 'model.model.12.m.1.cv1.conv.weight', 'model.model.12.m.1.cv1.bn.weight', 'model.model.12.m.1.cv1.bn.bias', 'model.model.12.m.1.cv1.bn.running_mean', 'model.model.12.m.1.cv1.bn.running_var', 'model.model.12.m.1.cv1.bn.num_batches_tracked', 'model.model.12.m.1.cv2.conv.weight', 'model.model.12.m.1.cv2.bn.weight', 'model.model.12.m.1.cv2.bn.bias', 'model.model.12.m.1.cv2.bn.running_mean', 'model.model.12.m.1.cv2.bn.running_var', 'model.model.12.m.1.cv2.bn.num_batches_tracked', 'model.model.12.m.2.cv1.conv.weight', 'model.model.12.m.2.cv1.bn.weight', 'model.model.12.m.2.cv1.bn.bias', 'model.model.12.m.2.cv1.bn.running_mean', 'model.model.12.m.2.cv1.bn.running_var', 'model.model.12.m.2.cv1.bn.num_batches_tracked', 'model.model.12.m.2.cv2.conv.weight', 'model.model.12.m.2.cv2.bn.weight', 'model.model.12.m.2.cv2.bn.bias', 'model.model.12.m.2.cv2.bn.running_mean', 'model.model.12.m.2.cv2.bn.running_var', 'model.model.12.m.2.cv2.bn.num_batches_tracked', 'model.model.15.cv1.conv.weight', 'model.model.15.cv1.bn.weight', 'model.model.15.cv1.bn.bias', 'model.model.15.cv1.bn.running_mean', 'model.model.15.cv1.bn.running_var', 'model.model.15.cv1.bn.num_batches_tracked', 'model.model.15.cv2.conv.weight', 'model.model.15.cv2.bn.weight', 'model.model.15.cv2.bn.bias', 'model.model.15.cv2.bn.running_mean', 'model.model.15.cv2.bn.running_var', 'model.model.15.cv2.bn.num_batches_tracked', 'model.model.15.m.0.cv1.conv.weight', 'model.model.15.m.0.cv1.bn.weight', 'model.model.15.m.0.cv1.bn.bias', 'model.model.15.m.0.cv1.bn.running_mean', 'model.model.15.m.0.cv1.bn.running_var', 'model.model.15.m.0.cv1.bn.num_batches_tracked', 'model.model.15.m.0.cv2.conv.weight', 'model.model.15.m.0.cv2.bn.weight', 'model.model.15.m.0.cv2.bn.bias', 'model.model.15.m.0.cv2.bn.running_mean', 'model.model.15.m.0.cv2.bn.running_var', 'model.model.15.m.0.cv2.bn.num_batches_tracked', 'model.model.15.m.1.cv1.conv.weight', 'model.model.15.m.1.cv1.bn.weight', 'model.model.15.m.1.cv1.bn.bias', 'model.model.15.m.1.cv1.bn.running_mean', 'model.model.15.m.1.cv1.bn.running_var', 'model.model.15.m.1.cv1.bn.num_batches_tracked', 'model.model.15.m.1.cv2.conv.weight', 'model.model.15.m.1.cv2.bn.weight', 'model.model.15.m.1.cv2.bn.bias', 'model.model.15.m.1.cv2.bn.running_mean', 'model.model.15.m.1.cv2.bn.running_var', 'model.model.15.m.1.cv2.bn.num_batches_tracked', 'model.model.15.m.2.cv1.conv.weight', 'model.model.15.m.2.cv1.bn.weight', 'model.model.15.m.2.cv1.bn.bias', 'model.model.15.m.2.cv1.bn.running_mean', 'model.model.15.m.2.cv1.bn.running_var', 'model.model.15.m.2.cv1.bn.num_batches_tracked', 'model.model.15.m.2.cv2.conv.weight', 'model.model.15.m.2.cv2.bn.weight', 'model.model.15.m.2.cv2.bn.bias', 'model.model.15.m.2.cv2.bn.running_mean', 'model.model.15.m.2.cv2.bn.running_var', 'model.model.15.m.2.cv2.bn.num_batches_tracked', 'model.model.16.conv.weight', 'model.model.16.bn.weight', 'model.model.16.bn.bias', 'model.model.16.bn.running_mean', 'model.model.16.bn.running_var', 'model.model.16.bn.num_batches_tracked', 'model.model.18.cv1.conv.weight', 'model.model.18.cv1.bn.weight', 'model.model.18.cv1.bn.bias', 'model.model.18.cv1.bn.running_mean', 'model.model.18.cv1.bn.running_var', 'model.model.18.cv1.bn.num_batches_tracked', 'model.model.18.cv2.conv.weight', 'model.model.18.cv2.bn.weight', 'model.model.18.cv2.bn.bias', 'model.model.18.cv2.bn.running_mean', 'model.model.18.cv2.bn.running_var', 'model.model.18.cv2.bn.num_batches_tracked', 'model.model.18.m.0.cv1.conv.weight', 'model.model.18.m.0.cv1.bn.weight', 'model.model.18.m.0.cv1.bn.bias', 'model.model.18.m.0.cv1.bn.running_mean', 'model.model.18.m.0.cv1.bn.running_var', 'model.model.18.m.0.cv1.bn.num_batches_tracked', 'model.model.18.m.0.cv2.conv.weight', 'model.model.18.m.0.cv2.bn.weight', 'model.model.18.m.0.cv2.bn.bias', 'model.model.18.m.0.cv2.bn.running_mean', 'model.model.18.m.0.cv2.bn.running_var', 'model.model.18.m.0.cv2.bn.num_batches_tracked', 'model.model.18.m.1.cv1.conv.weight', 'model.model.18.m.1.cv1.bn.weight', 'model.model.18.m.1.cv1.bn.bias', 'model.model.18.m.1.cv1.bn.running_mean', 'model.model.18.m.1.cv1.bn.running_var', 'model.model.18.m.1.cv1.bn.num_batches_tracked', 'model.model.18.m.1.cv2.conv.weight', 'model.model.18.m.1.cv2.bn.weight', 'model.model.18.m.1.cv2.bn.bias', 'model.model.18.m.1.cv2.bn.running_mean', 'model.model.18.m.1.cv2.bn.running_var', 'model.model.18.m.1.cv2.bn.num_batches_tracked', 'model.model.18.m.2.cv1.conv.weight', 'model.model.18.m.2.cv1.bn.weight', 'model.model.18.m.2.cv1.bn.bias', 'model.model.18.m.2.cv1.bn.running_mean', 'model.model.18.m.2.cv1.bn.running_var', 'model.model.18.m.2.cv1.bn.num_batches_tracked', 'model.model.18.m.2.cv2.conv.weight', 'model.model.18.m.2.cv2.bn.weight', 'model.model.18.m.2.cv2.bn.bias', 'model.model.18.m.2.cv2.bn.running_mean', 'model.model.18.m.2.cv2.bn.running_var', 'model.model.18.m.2.cv2.bn.num_batches_tracked', 'model.model.19.conv.weight', 'model.model.19.bn.weight', 'model.model.19.bn.bias', 'model.model.19.bn.running_mean', 'model.model.19.bn.running_var', 'model.model.19.bn.num_batches_tracked', 'model.model.21.cv1.conv.weight', 'model.model.21.cv1.bn.weight', 'model.model.21.cv1.bn.bias', 'model.model.21.cv1.bn.running_mean', 'model.model.21.cv1.bn.running_var', 'model.model.21.cv1.bn.num_batches_tracked', 'model.model.21.cv2.conv.weight', 'model.model.21.cv2.bn.weight', 'model.model.21.cv2.bn.bias', 'model.model.21.cv2.bn.running_mean', 'model.model.21.cv2.bn.running_var', 'model.model.21.cv2.bn.num_batches_tracked', 'model.model.21.m.0.cv1.conv.weight', 'model.model.21.m.0.cv1.bn.weight', 'model.model.21.m.0.cv1.bn.bias', 'model.model.21.m.0.cv1.bn.running_mean', 'model.model.21.m.0.cv1.bn.running_var', 'model.model.21.m.0.cv1.bn.num_batches_tracked', 'model.model.21.m.0.cv2.conv.weight', 'model.model.21.m.0.cv2.bn.weight', 'model.model.21.m.0.cv2.bn.bias', 'model.model.21.m.0.cv2.bn.running_mean', 'model.model.21.m.0.cv2.bn.running_var', 'model.model.21.m.0.cv2.bn.num_batches_tracked', 'model.model.21.m.1.cv1.conv.weight', 'model.model.21.m.1.cv1.bn.weight', 'model.model.21.m.1.cv1.bn.bias', 'model.model.21.m.1.cv1.bn.running_mean', 'model.model.21.m.1.cv1.bn.running_var', 'model.model.21.m.1.cv1.bn.num_batches_tracked', 'model.model.21.m.1.cv2.conv.weight', 'model.model.21.m.1.cv2.bn.weight', 'model.model.21.m.1.cv2.bn.bias', 'model.model.21.m.1.cv2.bn.running_mean', 'model.model.21.m.1.cv2.bn.running_var', 'model.model.21.m.1.cv2.bn.num_batches_tracked', 'model.model.21.m.2.cv1.conv.weight', 'model.model.21.m.2.cv1.bn.weight', 'model.model.21.m.2.cv1.bn.bias', 'model.model.21.m.2.cv1.bn.running_mean', 'model.model.21.m.2.cv1.bn.running_var', 'model.model.21.m.2.cv1.bn.num_batches_tracked', 'model.model.21.m.2.cv2.conv.weight', 'model.model.21.m.2.cv2.bn.weight', 'model.model.21.m.2.cv2.bn.bias', 'model.model.21.m.2.cv2.bn.running_mean', 'model.model.21.m.2.cv2.bn.running_var', 'model.model.21.m.2.cv2.bn.num_batches_tracked', 'model.model.22.cv2.0.0.conv.weight', 'model.model.22.cv2.0.0.bn.weight', 'model.model.22.cv2.0.0.bn.bias', 'model.model.22.cv2.0.0.bn.running_mean', 'model.model.22.cv2.0.0.bn.running_var', 'model.model.22.cv2.0.0.bn.num_batches_tracked', 'model.model.22.cv2.0.1.conv.weight', 'model.model.22.cv2.0.1.bn.weight', 'model.model.22.cv2.0.1.bn.bias', 'model.model.22.cv2.0.1.bn.running_mean', 'model.model.22.cv2.0.1.bn.running_var', 'model.model.22.cv2.0.1.bn.num_batches_tracked', 'model.model.22.cv2.0.2.weight', 'model.model.22.cv2.0.2.bias', 'model.model.22.cv2.1.0.conv.weight', 'model.model.22.cv2.1.0.bn.weight', 'model.model.22.cv2.1.0.bn.bias', 'model.model.22.cv2.1.0.bn.running_mean', 'model.model.22.cv2.1.0.bn.running_var', 'model.model.22.cv2.1.0.bn.num_batches_tracked', 'model.model.22.cv2.1.1.conv.weight', 'model.model.22.cv2.1.1.bn.weight', 'model.model.22.cv2.1.1.bn.bias', 'model.model.22.cv2.1.1.bn.running_mean', 'model.model.22.cv2.1.1.bn.running_var', 'model.model.22.cv2.1.1.bn.num_batches_tracked', 'model.model.22.cv2.1.2.weight', 'model.model.22.cv2.1.2.bias', 'model.model.22.cv2.2.0.conv.weight', 'model.model.22.cv2.2.0.bn.weight', 'model.model.22.cv2.2.0.bn.bias', 'model.model.22.cv2.2.0.bn.running_mean', 'model.model.22.cv2.2.0.bn.running_var', 'model.model.22.cv2.2.0.bn.num_batches_tracked', 'model.model.22.cv2.2.1.conv.weight', 'model.model.22.cv2.2.1.bn.weight', 'model.model.22.cv2.2.1.bn.bias', 'model.model.22.cv2.2.1.bn.running_mean', 'model.model.22.cv2.2.1.bn.running_var', 'model.model.22.cv2.2.1.bn.num_batches_tracked', 'model.model.22.cv2.2.2.weight', 'model.model.22.cv2.2.2.bias', 'model.model.22.cv3.0.0.conv.weight', 'model.model.22.cv3.0.0.bn.weight', 'model.model.22.cv3.0.0.bn.bias', 'model.model.22.cv3.0.0.bn.running_mean', 'model.model.22.cv3.0.0.bn.running_var', 'model.model.22.cv3.0.0.bn.num_batches_tracked', 'model.model.22.cv3.0.1.conv.weight', 'model.model.22.cv3.0.1.bn.weight', 'model.model.22.cv3.0.1.bn.bias', 'model.model.22.cv3.0.1.bn.running_mean', 'model.model.22.cv3.0.1.bn.running_var', 'model.model.22.cv3.0.1.bn.num_batches_tracked', 'model.model.22.cv3.0.2.weight', 'model.model.22.cv3.0.2.bias', 'model.model.22.cv3.1.0.conv.weight', 'model.model.22.cv3.1.0.bn.weight', 'model.model.22.cv3.1.0.bn.bias', 'model.model.22.cv3.1.0.bn.running_mean', 'model.model.22.cv3.1.0.bn.running_var', 'model.model.22.cv3.1.0.bn.num_batches_tracked', 'model.model.22.cv3.1.1.conv.weight', 'model.model.22.cv3.1.1.bn.weight', 'model.model.22.cv3.1.1.bn.bias', 'model.model.22.cv3.1.1.bn.running_mean', 'model.model.22.cv3.1.1.bn.running_var', 'model.model.22.cv3.1.1.bn.num_batches_tracked', 'model.model.22.cv3.1.2.weight', 'model.model.22.cv3.1.2.bias', 'model.model.22.cv3.2.0.conv.weight', 'model.model.22.cv3.2.0.bn.weight', 'model.model.22.cv3.2.0.bn.bias', 'model.model.22.cv3.2.0.bn.running_mean', 'model.model.22.cv3.2.0.bn.running_var', 'model.model.22.cv3.2.0.bn.num_batches_tracked', 'model.model.22.cv3.2.1.conv.weight', 'model.model.22.cv3.2.1.bn.weight', 'model.model.22.cv3.2.1.bn.bias', 'model.model.22.cv3.2.1.bn.running_mean', 'model.model.22.cv3.2.1.bn.running_var', 'model.model.22.cv3.2.1.bn.num_batches_tracked', 'model.model.22.cv3.2.2.weight', 'model.model.22.cv3.2.2.bias', 'model.model.22.dfl.conv.weight'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import torch\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "\n",
    "# Initialize pretrained model\n",
    "#model = YOLO('yolov8n.pt')\n",
    "\n",
    "model = YOLO(\"/home/areebadnan/Areeb_code/work/Atheritia/ultralytics/47logos_head_17logosbase_L/train5/weights/last.pt\")\n",
    "\n",
    "# Keep a copy of old state dict for sanity check\n",
    "old_dict = copy.deepcopy(model.state_dict())\n",
    "\n",
    "# We should freeze all but the last layer\n",
    "model.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 11\u001b[0m\n\u001b[1;32m      7\u001b[0m       module\u001b[38;5;241m.\u001b[39mtrack_running_stats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m      8\u001b[0m       \u001b[38;5;66;03m# print(name, \" put in eval mode.\")\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39madd_callback(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_train_epoch_start\u001b[39m\u001b[38;5;124m\"\u001b[39m, put_in_eval_mode)\n\u001b[1;32m     12\u001b[0m model\u001b[38;5;241m.\u001b[39madd_callback(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_pretrain_routine_start\u001b[39m\u001b[38;5;124m\"\u001b[39m, put_in_eval_mode)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Add a callback to put the frozen layers in eval mode to prevent BN values\n",
    "# from changing.\n",
    "def put_in_eval_mode(trainer, n_layers=22):\n",
    "  for i, (name, module) in enumerate(trainer.model.named_modules()):\n",
    "    if name.endswith(\"bn\") and int(name.split('.')[1]) < n_layers:\n",
    "      module.eval()\n",
    "      module.track_running_stats = False\n",
    "      # print(name, \" put in eval mode.\")\n",
    "\n",
    "\n",
    "model.add_callback(\"on_train_epoch_start\", put_in_eval_mode)\n",
    "model.add_callback(\"on_pretrain_routine_start\", put_in_eval_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.2.86 available ðŸ˜ƒ Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.2.55 ðŸš€ Python-3.10.12 torch-2.4.0+cu124 CUDA:0 (NVIDIA GeForce RTX 4060 Laptop GPU, 7721MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=/home/areebadnan/Areeb_code/work/Atheritia/ultralytics/47logos_head_17logosbase_L/train5/weights/last.pt, data=/home/areebadnan/Areeb_code/work/Atheritia/Datasets/47_logos_dataset/final_47_logos_dataset/data.yaml, epochs=500, time=None, patience=100, batch=6, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=47logos_head_17logosbase_L, name=train5, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=/home/areebadnan/Areeb_code/work/Atheritia/ultralytics/47logos_head_17logosbase_L/train5/weights/last.pt, amp=True, fraction=1.0, profile=False, freeze=22, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.0, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=47logos_head_17logosbase_L/train5\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir 47logos_head_17logosbase_L/train5', view at http://localhost:6006/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/areebadnan/Areeb_code/work/Atheritia/ultralytics/ultralytics/nn/tasks.py:785: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(file, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1856  ultralytics.nn.modules.conv.Conv             [3, 64, 3, 2]                 \n",
      "  1                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  2                  -1  3    279808  ultralytics.nn.modules.block.C2f             [128, 128, 3, True]           \n",
      "  3                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  4                  -1  6   2101248  ultralytics.nn.modules.block.C2f             [256, 256, 6, True]           \n",
      "  5                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  6                  -1  6   8396800  ultralytics.nn.modules.block.C2f             [512, 512, 6, True]           \n",
      "  7                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      "  8                  -1  3   4461568  ultralytics.nn.modules.block.C2f             [512, 512, 3, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  3   4723712  ultralytics.nn.modules.block.C2f             [1024, 512, 3]                \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  3   1247744  ultralytics.nn.modules.block.C2f             [768, 256, 3]                 \n",
      " 16                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  3   4592640  ultralytics.nn.modules.block.C2f             [768, 512, 3]                 \n",
      " 19                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  3   4723712  ultralytics.nn.modules.block.C2f             [1024, 512, 3]                \n",
      " 22        [15, 18, 21]  1   5619037  ultralytics.nn.modules.head.Detect           [47, [256, 512, 512]]         \n",
      "Model summary: 365 layers, 43,666,077 parameters, 43,666,061 gradients\n",
      "\n",
      "Transferred 595/595 items from pretrained weights\n",
      "Freezing layer 'model.0.conv.weight'\n",
      "Freezing layer 'model.0.bn.weight'\n",
      "Freezing layer 'model.0.bn.bias'\n",
      "Freezing layer 'model.1.conv.weight'\n",
      "Freezing layer 'model.1.bn.weight'\n",
      "Freezing layer 'model.1.bn.bias'\n",
      "Freezing layer 'model.2.cv1.conv.weight'\n",
      "Freezing layer 'model.2.cv1.bn.weight'\n",
      "Freezing layer 'model.2.cv1.bn.bias'\n",
      "Freezing layer 'model.2.cv2.conv.weight'\n",
      "Freezing layer 'model.2.cv2.bn.weight'\n",
      "Freezing layer 'model.2.cv2.bn.bias'\n",
      "Freezing layer 'model.2.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.2.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.2.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.2.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.2.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.2.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.2.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.2.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.2.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.2.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.2.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.2.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.2.m.2.cv1.conv.weight'\n",
      "Freezing layer 'model.2.m.2.cv1.bn.weight'\n",
      "Freezing layer 'model.2.m.2.cv1.bn.bias'\n",
      "Freezing layer 'model.2.m.2.cv2.conv.weight'\n",
      "Freezing layer 'model.2.m.2.cv2.bn.weight'\n",
      "Freezing layer 'model.2.m.2.cv2.bn.bias'\n",
      "Freezing layer 'model.3.conv.weight'\n",
      "Freezing layer 'model.3.bn.weight'\n",
      "Freezing layer 'model.3.bn.bias'\n",
      "Freezing layer 'model.4.cv1.conv.weight'\n",
      "Freezing layer 'model.4.cv1.bn.weight'\n",
      "Freezing layer 'model.4.cv1.bn.bias'\n",
      "Freezing layer 'model.4.cv2.conv.weight'\n",
      "Freezing layer 'model.4.cv2.bn.weight'\n",
      "Freezing layer 'model.4.cv2.bn.bias'\n",
      "Freezing layer 'model.4.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.4.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.4.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.4.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.4.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.4.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.4.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.4.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.4.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.4.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.4.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.4.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.4.m.2.cv1.conv.weight'\n",
      "Freezing layer 'model.4.m.2.cv1.bn.weight'\n",
      "Freezing layer 'model.4.m.2.cv1.bn.bias'\n",
      "Freezing layer 'model.4.m.2.cv2.conv.weight'\n",
      "Freezing layer 'model.4.m.2.cv2.bn.weight'\n",
      "Freezing layer 'model.4.m.2.cv2.bn.bias'\n",
      "Freezing layer 'model.4.m.3.cv1.conv.weight'\n",
      "Freezing layer 'model.4.m.3.cv1.bn.weight'\n",
      "Freezing layer 'model.4.m.3.cv1.bn.bias'\n",
      "Freezing layer 'model.4.m.3.cv2.conv.weight'\n",
      "Freezing layer 'model.4.m.3.cv2.bn.weight'\n",
      "Freezing layer 'model.4.m.3.cv2.bn.bias'\n",
      "Freezing layer 'model.4.m.4.cv1.conv.weight'\n",
      "Freezing layer 'model.4.m.4.cv1.bn.weight'\n",
      "Freezing layer 'model.4.m.4.cv1.bn.bias'\n",
      "Freezing layer 'model.4.m.4.cv2.conv.weight'\n",
      "Freezing layer 'model.4.m.4.cv2.bn.weight'\n",
      "Freezing layer 'model.4.m.4.cv2.bn.bias'\n",
      "Freezing layer 'model.4.m.5.cv1.conv.weight'\n",
      "Freezing layer 'model.4.m.5.cv1.bn.weight'\n",
      "Freezing layer 'model.4.m.5.cv1.bn.bias'\n",
      "Freezing layer 'model.4.m.5.cv2.conv.weight'\n",
      "Freezing layer 'model.4.m.5.cv2.bn.weight'\n",
      "Freezing layer 'model.4.m.5.cv2.bn.bias'\n",
      "Freezing layer 'model.5.conv.weight'\n",
      "Freezing layer 'model.5.bn.weight'\n",
      "Freezing layer 'model.5.bn.bias'\n",
      "Freezing layer 'model.6.cv1.conv.weight'\n",
      "Freezing layer 'model.6.cv1.bn.weight'\n",
      "Freezing layer 'model.6.cv1.bn.bias'\n",
      "Freezing layer 'model.6.cv2.conv.weight'\n",
      "Freezing layer 'model.6.cv2.bn.weight'\n",
      "Freezing layer 'model.6.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.2.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.2.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.2.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.2.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.2.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.2.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.3.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.3.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.3.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.3.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.3.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.3.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.4.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.4.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.4.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.4.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.4.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.4.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.5.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.5.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.5.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.5.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.5.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.5.cv2.bn.bias'\n",
      "Freezing layer 'model.7.conv.weight'\n",
      "Freezing layer 'model.7.bn.weight'\n",
      "Freezing layer 'model.7.bn.bias'\n",
      "Freezing layer 'model.8.cv1.conv.weight'\n",
      "Freezing layer 'model.8.cv1.bn.weight'\n",
      "Freezing layer 'model.8.cv1.bn.bias'\n",
      "Freezing layer 'model.8.cv2.conv.weight'\n",
      "Freezing layer 'model.8.cv2.bn.weight'\n",
      "Freezing layer 'model.8.cv2.bn.bias'\n",
      "Freezing layer 'model.8.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.8.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.8.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.8.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.8.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.8.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.8.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.8.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.8.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.8.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.8.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.8.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.8.m.2.cv1.conv.weight'\n",
      "Freezing layer 'model.8.m.2.cv1.bn.weight'\n",
      "Freezing layer 'model.8.m.2.cv1.bn.bias'\n",
      "Freezing layer 'model.8.m.2.cv2.conv.weight'\n",
      "Freezing layer 'model.8.m.2.cv2.bn.weight'\n",
      "Freezing layer 'model.8.m.2.cv2.bn.bias'\n",
      "Freezing layer 'model.9.cv1.conv.weight'\n",
      "Freezing layer 'model.9.cv1.bn.weight'\n",
      "Freezing layer 'model.9.cv1.bn.bias'\n",
      "Freezing layer 'model.9.cv2.conv.weight'\n",
      "Freezing layer 'model.9.cv2.bn.weight'\n",
      "Freezing layer 'model.9.cv2.bn.bias'\n",
      "Freezing layer 'model.12.cv1.conv.weight'\n",
      "Freezing layer 'model.12.cv1.bn.weight'\n",
      "Freezing layer 'model.12.cv1.bn.bias'\n",
      "Freezing layer 'model.12.cv2.conv.weight'\n",
      "Freezing layer 'model.12.cv2.bn.weight'\n",
      "Freezing layer 'model.12.cv2.bn.bias'\n",
      "Freezing layer 'model.12.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.12.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.12.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.12.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.12.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.12.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.12.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.12.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.12.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.12.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.12.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.12.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.12.m.2.cv1.conv.weight'\n",
      "Freezing layer 'model.12.m.2.cv1.bn.weight'\n",
      "Freezing layer 'model.12.m.2.cv1.bn.bias'\n",
      "Freezing layer 'model.12.m.2.cv2.conv.weight'\n",
      "Freezing layer 'model.12.m.2.cv2.bn.weight'\n",
      "Freezing layer 'model.12.m.2.cv2.bn.bias'\n",
      "Freezing layer 'model.15.cv1.conv.weight'\n",
      "Freezing layer 'model.15.cv1.bn.weight'\n",
      "Freezing layer 'model.15.cv1.bn.bias'\n",
      "Freezing layer 'model.15.cv2.conv.weight'\n",
      "Freezing layer 'model.15.cv2.bn.weight'\n",
      "Freezing layer 'model.15.cv2.bn.bias'\n",
      "Freezing layer 'model.15.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.15.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.15.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.15.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.15.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.15.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.15.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.15.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.15.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.15.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.15.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.15.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.15.m.2.cv1.conv.weight'\n",
      "Freezing layer 'model.15.m.2.cv1.bn.weight'\n",
      "Freezing layer 'model.15.m.2.cv1.bn.bias'\n",
      "Freezing layer 'model.15.m.2.cv2.conv.weight'\n",
      "Freezing layer 'model.15.m.2.cv2.bn.weight'\n",
      "Freezing layer 'model.15.m.2.cv2.bn.bias'\n",
      "Freezing layer 'model.16.conv.weight'\n",
      "Freezing layer 'model.16.bn.weight'\n",
      "Freezing layer 'model.16.bn.bias'\n",
      "Freezing layer 'model.18.cv1.conv.weight'\n",
      "Freezing layer 'model.18.cv1.bn.weight'\n",
      "Freezing layer 'model.18.cv1.bn.bias'\n",
      "Freezing layer 'model.18.cv2.conv.weight'\n",
      "Freezing layer 'model.18.cv2.bn.weight'\n",
      "Freezing layer 'model.18.cv2.bn.bias'\n",
      "Freezing layer 'model.18.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.18.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.18.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.18.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.18.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.18.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.18.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.18.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.18.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.18.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.18.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.18.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.18.m.2.cv1.conv.weight'\n",
      "Freezing layer 'model.18.m.2.cv1.bn.weight'\n",
      "Freezing layer 'model.18.m.2.cv1.bn.bias'\n",
      "Freezing layer 'model.18.m.2.cv2.conv.weight'\n",
      "Freezing layer 'model.18.m.2.cv2.bn.weight'\n",
      "Freezing layer 'model.18.m.2.cv2.bn.bias'\n",
      "Freezing layer 'model.19.conv.weight'\n",
      "Freezing layer 'model.19.bn.weight'\n",
      "Freezing layer 'model.19.bn.bias'\n",
      "Freezing layer 'model.21.cv1.conv.weight'\n",
      "Freezing layer 'model.21.cv1.bn.weight'\n",
      "Freezing layer 'model.21.cv1.bn.bias'\n",
      "Freezing layer 'model.21.cv2.conv.weight'\n",
      "Freezing layer 'model.21.cv2.bn.weight'\n",
      "Freezing layer 'model.21.cv2.bn.bias'\n",
      "Freezing layer 'model.21.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.21.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.21.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.21.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.21.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.21.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.21.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.21.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.21.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.21.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.21.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.21.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.21.m.2.cv1.conv.weight'\n",
      "Freezing layer 'model.21.m.2.cv1.bn.weight'\n",
      "Freezing layer 'model.21.m.2.cv1.bn.bias'\n",
      "Freezing layer 'model.21.m.2.cv2.conv.weight'\n",
      "Freezing layer 'model.21.m.2.cv2.bn.weight'\n",
      "Freezing layer 'model.21.m.2.cv2.bn.bias'\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/areebadnan/Areeb_code/work/Atheritia/ultralytics/ultralytics/utils/checks.py:651: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(True):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/areebadnan/Areeb_code/work/Atheritia/ultralytics/ultralytics/engine/trainer.py:267: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(enabled=self.amp)\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/areebadnan/Areeb_code/work/Atheritia/Datasets/47_logos_dataset/final_47_logos_dataset/train/combined_aug_org/labels.cache... 73630 images, 156 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 73630/73630 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/areebadnan/Areeb_code/work/Atheritia/Datasets/47_logos_dataset/final_47_logos_dataset/valid/labels.cache... 9738 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9738/9738 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to 47logos_head_17logosbase_L/train5/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01, momentum=0.9) with parameter groups 97 weight(decay=0.0), 104 weight(decay=0.000515625), 103 bias(decay=0.0)\n",
      "Resuming training /home/areebadnan/Areeb_code/work/Atheritia/ultralytics/47logos_head_17logosbase_L/train5/weights/last.pt from epoch 273 to 500 total epochs\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1m47logos_head_17logosbase_L/train5\u001b[0m\n",
      "Starting training for 500 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    273/500      1.29G      1.283     0.8607      1.209         50        640:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 9837/12272 [15:29<03:49, 10.59it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Areeb_code/work/Atheritia/ultralytics/ultralytics/engine/model.py:650\u001b[0m, in \u001b[0;36mModel.train\u001b[0;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m    649\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mhub_session \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession  \u001b[38;5;66;03m# attach optional HUB session\u001b[39;00m\n\u001b[0;32m--> 650\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[38;5;66;03m# Update model and cfg after training\u001b[39;00m\n\u001b[1;32m    652\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m}:\n",
      "File \u001b[0;32m~/Areeb_code/work/Atheritia/ultralytics/ultralytics/engine/trainer.py:204\u001b[0m, in \u001b[0;36mBaseTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    201\u001b[0m         ddp_cleanup(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mstr\u001b[39m(file))\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 204\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Areeb_code/work/Atheritia/ultralytics/ultralytics/engine/trainer.py:381\u001b[0m, in \u001b[0;36mBaseTrainer._do_train\u001b[0;34m(self, world_size)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mamp):\n\u001b[1;32m    380\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess_batch(batch)\n\u001b[0;32m--> 381\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_items \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    382\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    383\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m world_size\n",
      "File \u001b[0;32m~/Areeb_Python_Environments/yolo_env1/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Areeb_Python_Environments/yolo_env1/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Areeb_code/work/Atheritia/ultralytics/ultralytics/nn/tasks.py:102\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;124;03mForward pass of the model on a single scale. Wrapper for `_forward_once` method.\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    (torch.Tensor): The output of the network.\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Areeb_code/work/Atheritia/ultralytics/ultralytics/nn/tasks.py:285\u001b[0m, in \u001b[0;36mBaseModel.loss\u001b[0;34m(self, batch, preds)\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_criterion()\n\u001b[1;32m    284\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m preds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m preds\n\u001b[0;32m--> 285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Areeb_code/work/Atheritia/ultralytics/ultralytics/utils/loss.py:219\u001b[0m, in \u001b[0;36mv8DetectionLoss.__call__\u001b[0;34m(self, preds, batch)\u001b[0m\n\u001b[1;32m    217\u001b[0m dtype \u001b[38;5;241m=\u001b[39m pred_scores\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m    218\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m pred_scores\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 219\u001b[0m imgsz \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeats\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# image size (h,w)\u001b[39;00m\n\u001b[1;32m    220\u001b[0m anchor_points, stride_tensor \u001b[38;5;241m=\u001b[39m make_anchors(feats, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, \u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m    222\u001b[0m \u001b[38;5;66;03m# Targets\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results = model.train(resume=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.2.81 available ðŸ˜ƒ Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.2.55 ðŸš€ Python-3.10.12 torch-2.4.0+cu124 CUDA:0 (NVIDIA GeForce RTX 4060 Laptop GPU, 7721MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=/home/areebadnan/Areeb_code/work/Atheritia/All_models/Large/coco_base_finetune_17logo_L.pt, data=/home/areebadnan/Areeb_code/work/Atheritia/Datasets/47_logos_dataset/final_47_logos_dataset/data.yaml, epochs=500, time=None, patience=100, batch=-1, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=47logos_head_17logosbase_L, name=train5, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=22, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=47logos_head_17logosbase_L/train5\n",
      "Overriding model.yaml nc=17 with nc=47\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1856  ultralytics.nn.modules.conv.Conv             [3, 64, 3, 2]                 \n",
      "  1                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  2                  -1  3    279808  ultralytics.nn.modules.block.C2f             [128, 128, 3, True]           \n",
      "  3                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  4                  -1  6   2101248  ultralytics.nn.modules.block.C2f             [256, 256, 6, True]           \n",
      "  5                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  6                  -1  6   8396800  ultralytics.nn.modules.block.C2f             [512, 512, 6, True]           \n",
      "  7                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      "  8                  -1  3   4461568  ultralytics.nn.modules.block.C2f             [512, 512, 3, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  3   4723712  ultralytics.nn.modules.block.C2f             [1024, 512, 3]                \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  3   1247744  ultralytics.nn.modules.block.C2f             [768, 256, 3]                 \n",
      " 16                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  3   4592640  ultralytics.nn.modules.block.C2f             [768, 512, 3]                 \n",
      " 19                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  3   4723712  ultralytics.nn.modules.block.C2f             [1024, 512, 3]                \n",
      " 22        [15, 18, 21]  1   5619037  ultralytics.nn.modules.head.Detect           [47, [256, 512, 512]]         \n",
      "Model summary: 365 layers, 43,666,077 parameters, 43,666,061 gradients\n",
      "\n",
      "Transferred 589/595 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir 47logos_head_17logosbase_L/train5', view at http://localhost:6006/\n",
      "Freezing layer 'model.0.conv.weight'\n",
      "Freezing layer 'model.0.bn.weight'\n",
      "Freezing layer 'model.0.bn.bias'\n",
      "Freezing layer 'model.1.conv.weight'\n",
      "Freezing layer 'model.1.bn.weight'\n",
      "Freezing layer 'model.1.bn.bias'\n",
      "Freezing layer 'model.2.cv1.conv.weight'\n",
      "Freezing layer 'model.2.cv1.bn.weight'\n",
      "Freezing layer 'model.2.cv1.bn.bias'\n",
      "Freezing layer 'model.2.cv2.conv.weight'\n",
      "Freezing layer 'model.2.cv2.bn.weight'\n",
      "Freezing layer 'model.2.cv2.bn.bias'\n",
      "Freezing layer 'model.2.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.2.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.2.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.2.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.2.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.2.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.2.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.2.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.2.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.2.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.2.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.2.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.2.m.2.cv1.conv.weight'\n",
      "Freezing layer 'model.2.m.2.cv1.bn.weight'\n",
      "Freezing layer 'model.2.m.2.cv1.bn.bias'\n",
      "Freezing layer 'model.2.m.2.cv2.conv.weight'\n",
      "Freezing layer 'model.2.m.2.cv2.bn.weight'\n",
      "Freezing layer 'model.2.m.2.cv2.bn.bias'\n",
      "Freezing layer 'model.3.conv.weight'\n",
      "Freezing layer 'model.3.bn.weight'\n",
      "Freezing layer 'model.3.bn.bias'\n",
      "Freezing layer 'model.4.cv1.conv.weight'\n",
      "Freezing layer 'model.4.cv1.bn.weight'\n",
      "Freezing layer 'model.4.cv1.bn.bias'\n",
      "Freezing layer 'model.4.cv2.conv.weight'\n",
      "Freezing layer 'model.4.cv2.bn.weight'\n",
      "Freezing layer 'model.4.cv2.bn.bias'\n",
      "Freezing layer 'model.4.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.4.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.4.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.4.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.4.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.4.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.4.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.4.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.4.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.4.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.4.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.4.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.4.m.2.cv1.conv.weight'\n",
      "Freezing layer 'model.4.m.2.cv1.bn.weight'\n",
      "Freezing layer 'model.4.m.2.cv1.bn.bias'\n",
      "Freezing layer 'model.4.m.2.cv2.conv.weight'\n",
      "Freezing layer 'model.4.m.2.cv2.bn.weight'\n",
      "Freezing layer 'model.4.m.2.cv2.bn.bias'\n",
      "Freezing layer 'model.4.m.3.cv1.conv.weight'\n",
      "Freezing layer 'model.4.m.3.cv1.bn.weight'\n",
      "Freezing layer 'model.4.m.3.cv1.bn.bias'\n",
      "Freezing layer 'model.4.m.3.cv2.conv.weight'\n",
      "Freezing layer 'model.4.m.3.cv2.bn.weight'\n",
      "Freezing layer 'model.4.m.3.cv2.bn.bias'\n",
      "Freezing layer 'model.4.m.4.cv1.conv.weight'\n",
      "Freezing layer 'model.4.m.4.cv1.bn.weight'\n",
      "Freezing layer 'model.4.m.4.cv1.bn.bias'\n",
      "Freezing layer 'model.4.m.4.cv2.conv.weight'\n",
      "Freezing layer 'model.4.m.4.cv2.bn.weight'\n",
      "Freezing layer 'model.4.m.4.cv2.bn.bias'\n",
      "Freezing layer 'model.4.m.5.cv1.conv.weight'\n",
      "Freezing layer 'model.4.m.5.cv1.bn.weight'\n",
      "Freezing layer 'model.4.m.5.cv1.bn.bias'\n",
      "Freezing layer 'model.4.m.5.cv2.conv.weight'\n",
      "Freezing layer 'model.4.m.5.cv2.bn.weight'\n",
      "Freezing layer 'model.4.m.5.cv2.bn.bias'\n",
      "Freezing layer 'model.5.conv.weight'\n",
      "Freezing layer 'model.5.bn.weight'\n",
      "Freezing layer 'model.5.bn.bias'\n",
      "Freezing layer 'model.6.cv1.conv.weight'\n",
      "Freezing layer 'model.6.cv1.bn.weight'\n",
      "Freezing layer 'model.6.cv1.bn.bias'\n",
      "Freezing layer 'model.6.cv2.conv.weight'\n",
      "Freezing layer 'model.6.cv2.bn.weight'\n",
      "Freezing layer 'model.6.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.2.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.2.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.2.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.2.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.2.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.2.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.3.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.3.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.3.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.3.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.3.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.3.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.4.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.4.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.4.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.4.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.4.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.4.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.5.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.5.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.5.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.5.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.5.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.5.cv2.bn.bias'\n",
      "Freezing layer 'model.7.conv.weight'\n",
      "Freezing layer 'model.7.bn.weight'\n",
      "Freezing layer 'model.7.bn.bias'\n",
      "Freezing layer 'model.8.cv1.conv.weight'\n",
      "Freezing layer 'model.8.cv1.bn.weight'\n",
      "Freezing layer 'model.8.cv1.bn.bias'\n",
      "Freezing layer 'model.8.cv2.conv.weight'\n",
      "Freezing layer 'model.8.cv2.bn.weight'\n",
      "Freezing layer 'model.8.cv2.bn.bias'\n",
      "Freezing layer 'model.8.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.8.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.8.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.8.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.8.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.8.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.8.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.8.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.8.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.8.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.8.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.8.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.8.m.2.cv1.conv.weight'\n",
      "Freezing layer 'model.8.m.2.cv1.bn.weight'\n",
      "Freezing layer 'model.8.m.2.cv1.bn.bias'\n",
      "Freezing layer 'model.8.m.2.cv2.conv.weight'\n",
      "Freezing layer 'model.8.m.2.cv2.bn.weight'\n",
      "Freezing layer 'model.8.m.2.cv2.bn.bias'\n",
      "Freezing layer 'model.9.cv1.conv.weight'\n",
      "Freezing layer 'model.9.cv1.bn.weight'\n",
      "Freezing layer 'model.9.cv1.bn.bias'\n",
      "Freezing layer 'model.9.cv2.conv.weight'\n",
      "Freezing layer 'model.9.cv2.bn.weight'\n",
      "Freezing layer 'model.9.cv2.bn.bias'\n",
      "Freezing layer 'model.12.cv1.conv.weight'\n",
      "Freezing layer 'model.12.cv1.bn.weight'\n",
      "Freezing layer 'model.12.cv1.bn.bias'\n",
      "Freezing layer 'model.12.cv2.conv.weight'\n",
      "Freezing layer 'model.12.cv2.bn.weight'\n",
      "Freezing layer 'model.12.cv2.bn.bias'\n",
      "Freezing layer 'model.12.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.12.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.12.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.12.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.12.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.12.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.12.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.12.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.12.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.12.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.12.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.12.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.12.m.2.cv1.conv.weight'\n",
      "Freezing layer 'model.12.m.2.cv1.bn.weight'\n",
      "Freezing layer 'model.12.m.2.cv1.bn.bias'\n",
      "Freezing layer 'model.12.m.2.cv2.conv.weight'\n",
      "Freezing layer 'model.12.m.2.cv2.bn.weight'\n",
      "Freezing layer 'model.12.m.2.cv2.bn.bias'\n",
      "Freezing layer 'model.15.cv1.conv.weight'\n",
      "Freezing layer 'model.15.cv1.bn.weight'\n",
      "Freezing layer 'model.15.cv1.bn.bias'\n",
      "Freezing layer 'model.15.cv2.conv.weight'\n",
      "Freezing layer 'model.15.cv2.bn.weight'\n",
      "Freezing layer 'model.15.cv2.bn.bias'\n",
      "Freezing layer 'model.15.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.15.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.15.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.15.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.15.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.15.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.15.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.15.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.15.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.15.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.15.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.15.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.15.m.2.cv1.conv.weight'\n",
      "Freezing layer 'model.15.m.2.cv1.bn.weight'\n",
      "Freezing layer 'model.15.m.2.cv1.bn.bias'\n",
      "Freezing layer 'model.15.m.2.cv2.conv.weight'\n",
      "Freezing layer 'model.15.m.2.cv2.bn.weight'\n",
      "Freezing layer 'model.15.m.2.cv2.bn.bias'\n",
      "Freezing layer 'model.16.conv.weight'\n",
      "Freezing layer 'model.16.bn.weight'\n",
      "Freezing layer 'model.16.bn.bias'\n",
      "Freezing layer 'model.18.cv1.conv.weight'\n",
      "Freezing layer 'model.18.cv1.bn.weight'\n",
      "Freezing layer 'model.18.cv1.bn.bias'\n",
      "Freezing layer 'model.18.cv2.conv.weight'\n",
      "Freezing layer 'model.18.cv2.bn.weight'\n",
      "Freezing layer 'model.18.cv2.bn.bias'\n",
      "Freezing layer 'model.18.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.18.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.18.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.18.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.18.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.18.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.18.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.18.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.18.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.18.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.18.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.18.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.18.m.2.cv1.conv.weight'\n",
      "Freezing layer 'model.18.m.2.cv1.bn.weight'\n",
      "Freezing layer 'model.18.m.2.cv1.bn.bias'\n",
      "Freezing layer 'model.18.m.2.cv2.conv.weight'\n",
      "Freezing layer 'model.18.m.2.cv2.bn.weight'\n",
      "Freezing layer 'model.18.m.2.cv2.bn.bias'\n",
      "Freezing layer 'model.19.conv.weight'\n",
      "Freezing layer 'model.19.bn.weight'\n",
      "Freezing layer 'model.19.bn.bias'\n",
      "Freezing layer 'model.21.cv1.conv.weight'\n",
      "Freezing layer 'model.21.cv1.bn.weight'\n",
      "Freezing layer 'model.21.cv1.bn.bias'\n",
      "Freezing layer 'model.21.cv2.conv.weight'\n",
      "Freezing layer 'model.21.cv2.bn.weight'\n",
      "Freezing layer 'model.21.cv2.bn.bias'\n",
      "Freezing layer 'model.21.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.21.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.21.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.21.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.21.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.21.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.21.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.21.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.21.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.21.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.21.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.21.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.21.m.2.cv1.conv.weight'\n",
      "Freezing layer 'model.21.m.2.cv1.bn.weight'\n",
      "Freezing layer 'model.21.m.2.cv1.bn.bias'\n",
      "Freezing layer 'model.21.m.2.cv2.conv.weight'\n",
      "Freezing layer 'model.21.m.2.cv2.bn.weight'\n",
      "Freezing layer 'model.21.m.2.cv2.bn.bias'\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/areebadnan/Areeb_code/work/Atheritia/ultralytics/ultralytics/nn/tasks.py:785: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(file, map_location=\"cpu\")\n",
      "/home/areebadnan/Areeb_code/work/Atheritia/ultralytics/ultralytics/utils/checks.py:651: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(True):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
      "\u001b[34m\u001b[1mAutoBatch: \u001b[0mComputing optimal batch size for imgsz=640 at 60.0% CUDA memory utilization.\n",
      "\u001b[34m\u001b[1mAutoBatch: \u001b[0mCUDA:0 (NVIDIA GeForce RTX 4060 Laptop GPU) 7.54G total, 0.38G reserved, 0.37G allocated, 6.78G free\n",
      "      Params      GFLOPs  GPU_mem (GB)  forward (ms) backward (ms)                   input                  output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/areebadnan/Areeb_code/work/Atheritia/ultralytics/ultralytics/engine/trainer.py:267: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(enabled=self.amp)\n",
      "/home/areebadnan/Areeb_code/work/Atheritia/ultralytics/ultralytics/utils/autobatch.py:26: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    43666077           0         1.067          36.5         64.18        (1, 3, 640, 640)                    list\n",
      "    43666077           0         1.638         48.64         49.57        (2, 3, 640, 640)                    list\n",
      "    43666077           0         2.787         89.13         97.39        (4, 3, 640, 640)                    list\n",
      "    43666077           0         5.035         181.3           204        (8, 3, 640, 640)                    list\n",
      "CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacity of 7.54 GiB of which 4.56 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.19 GiB is allocated by PyTorch, and 172.14 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mAutoBatch: \u001b[0mUsing batch-size 6 for CUDA:0 4.66G/7.54G (62%) âœ…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/areebadnan/Areeb_code/work/Atheritia/Datasets/47_logos_dataset/final_47_logos_dataset/train/combined_aug_org/labels.cache... 73630 images, 156 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 73630/73630 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/areebadnan/Areeb_code/work/Atheritia/Datasets/47_logos_dataset/final_47_logos_dataset/valid/labels... 9738 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9738/9738 [00:02<00:00, 3788.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /home/areebadnan/Areeb_code/work/Atheritia/Datasets/47_logos_dataset/final_47_logos_dataset/valid/labels.cache\n",
      "Plotting labels to 47logos_head_17logosbase_L/train5/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01, momentum=0.9) with parameter groups 97 weight(decay=0.0), 104 weight(decay=0.000515625), 103 bias(decay=0.0)\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1m47logos_head_17logosbase_L/train5\u001b[0m\n",
      "Starting training for 500 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      1/500      1.12G      1.949      2.274      1.569         20        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12272/12272 [20:04<00:00, 10.19it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 812/812 [01:28<00:00,  9.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       9738      32413      0.117      0.123     0.0997     0.0433\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      2/500      1.14G      1.635      1.446      1.412         18        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12272/12272 [19:27<00:00, 10.51it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 812/812 [01:27<00:00,  9.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       9738      32413      0.109      0.125      0.106     0.0527\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      3/500      1.14G      1.542      1.308      1.358         19        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12272/12272 [19:20<00:00, 10.58it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 812/812 [01:27<00:00,  9.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       9738      32413      0.139      0.122       0.11     0.0542\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      4/500       1.1G      1.494      1.226      1.334         38        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12272/12272 [19:16<00:00, 10.61it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 812/812 [01:27<00:00,  9.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       9738      32413      0.124      0.142      0.108     0.0535\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      5/500      1.15G      1.455      1.157      1.312         24        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12272/12272 [19:17<00:00, 10.61it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 812/812 [01:27<00:00,  9.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       9738      32413      0.126      0.141      0.109     0.0558\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      6/500      1.07G      1.433       1.12      1.301         15        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12272/12272 [19:15<00:00, 10.62it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 812/812 [01:27<00:00,  9.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       9738      32413      0.113      0.137       0.11     0.0566\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      7/500       1.1G      1.418      1.096       1.29         26        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12272/12272 [19:16<00:00, 10.62it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 812/812 [01:27<00:00,  9.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       9738      32413      0.106      0.146      0.113     0.0578\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      8/500      1.06G        1.4      1.067      1.283         23        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12272/12272 [19:13<00:00, 10.64it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 812/812 [01:27<00:00,  9.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       9738      32413      0.109      0.144      0.113     0.0586\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      9/500       1.1G      1.393      1.058      1.281         37        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12272/12272 [19:12<00:00, 10.65it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 812/812 [01:27<00:00,  9.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       9738      32413      0.132      0.146      0.115     0.0594\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     10/500      1.02G      1.387      1.043      1.274         17        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12272/12272 [19:15<00:00, 10.62it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 812/812 [01:27<00:00,  9.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       9738      32413      0.132      0.145      0.117     0.0609\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     11/500       1.1G       1.38      1.033      1.272         15        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12272/12272 [19:18<00:00, 10.60it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 812/812 [01:27<00:00,  9.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       9738      32413      0.132       0.15      0.119     0.0619\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     12/500      1.14G      1.377      1.029       1.27         15        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12272/12272 [19:15<00:00, 10.62it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 812/812 [01:27<00:00,  9.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       9738      32413      0.132       0.15      0.119     0.0623\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     13/500       1.1G      1.372      1.021      1.266         15        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12272/12272 [19:16<00:00, 10.61it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 812/812 [01:27<00:00,  9.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       9738      32413      0.132      0.151      0.119     0.0624\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     14/500      1.12G      1.368      1.016      1.265         30        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12272/12272 [19:16<00:00, 10.61it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 812/812 [01:27<00:00,  9.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       9738      32413      0.132      0.151       0.12     0.0629\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     15/500      1.12G      1.365       1.01      1.263         22        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12272/12272 [21:42<00:00,  9.42it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 812/812 [01:52<00:00,  7.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       9738      32413      0.132      0.152       0.12     0.0629\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     16/500      1.02G      1.366     0.9973      1.258         39        640:  26%|â–ˆâ–ˆâ–‹       | 3226/12272 [2:16:33<6:22:56,  2.54s/it]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/home/areebadnan/Areeb_code/work/Atheritia/Datasets/47_logos_dataset/final_47_logos_dataset/data.yaml\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreeze\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m22\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m640\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproject\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m47logos_head_17logosbase_L\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Areeb_code/work/Atheritia/ultralytics/ultralytics/engine/model.py:650\u001b[0m, in \u001b[0;36mModel.train\u001b[0;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m    649\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mhub_session \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession  \u001b[38;5;66;03m# attach optional HUB session\u001b[39;00m\n\u001b[0;32m--> 650\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[38;5;66;03m# Update model and cfg after training\u001b[39;00m\n\u001b[1;32m    652\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m}:\n",
      "File \u001b[0;32m~/Areeb_code/work/Atheritia/ultralytics/ultralytics/engine/trainer.py:204\u001b[0m, in \u001b[0;36mBaseTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    201\u001b[0m         ddp_cleanup(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mstr\u001b[39m(file))\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 204\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Areeb_code/work/Atheritia/ultralytics/ultralytics/engine/trainer.py:381\u001b[0m, in \u001b[0;36mBaseTrainer._do_train\u001b[0;34m(self, world_size)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mamp):\n\u001b[1;32m    380\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess_batch(batch)\n\u001b[0;32m--> 381\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_items \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    382\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    383\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m world_size\n",
      "File \u001b[0;32m~/Areeb_Python_Environments/yolo_env1/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Areeb_Python_Environments/yolo_env1/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Areeb_code/work/Atheritia/ultralytics/ultralytics/nn/tasks.py:102\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;124;03mForward pass of the model on a single scale. Wrapper for `_forward_once` method.\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    (torch.Tensor): The output of the network.\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Areeb_code/work/Atheritia/ultralytics/ultralytics/nn/tasks.py:285\u001b[0m, in \u001b[0;36mBaseModel.loss\u001b[0;34m(self, batch, preds)\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_criterion()\n\u001b[1;32m    284\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m preds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m preds\n\u001b[0;32m--> 285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Areeb_code/work/Atheritia/ultralytics/ultralytics/utils/loss.py:219\u001b[0m, in \u001b[0;36mv8DetectionLoss.__call__\u001b[0;34m(self, preds, batch)\u001b[0m\n\u001b[1;32m    217\u001b[0m dtype \u001b[38;5;241m=\u001b[39m pred_scores\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m    218\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m pred_scores\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 219\u001b[0m imgsz \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeats\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# image size (h,w)\u001b[39;00m\n\u001b[1;32m    220\u001b[0m anchor_points, stride_tensor \u001b[38;5;241m=\u001b[39m make_anchors(feats, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, \u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m    222\u001b[0m \u001b[38;5;66;03m# Targets\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "results = model.train(data='/home/areebadnan/Areeb_code/work/Atheritia/Datasets/47_logos_dataset/final_47_logos_dataset/data.yaml', freeze=22, epochs=500, imgsz=640, batch=-1, project='47logos_head_17logosbase_L')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo_env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

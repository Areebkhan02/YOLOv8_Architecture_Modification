{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'ultralytics'...\n",
      "remote: Enumerating objects: 42361, done.\u001b[K\n",
      "remote: Counting objects: 100% (1928/1928), done.\u001b[K\n",
      "remote: Compressing objects: 100% (1024/1024), done.\u001b[K\n",
      "remote: Total 42361 (delta 1222), reused 1448 (delta 895), pack-reused 40433 (from 1)\u001b[K\n",
      "Receiving objects: 100% (42361/42361), 32.27 MiB | 1.56 MiB/s, done.\n",
      "Resolving deltas: 100% (31181/31181), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/ultralytics/ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/areebadnan/Areeb_code/work/Atheritia/Notebooks/YOLOv8_Architecture_Modification/YOLOv8_Architecture_Modification/Pose_Head_Addition/ultralytics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/areebadnan/Areeb_Python_Environments/yolo_env1/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///home/areebadnan/Areeb_code/work/Atheritia/Notebooks/YOLOv8_Architecture_Modification/YOLOv8_Architecture_Modification/Pose_Head_Addition/ultralytics\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: seaborn>=0.11.0 in /home/areebadnan/Areeb_Python_Environments/yolo_env1/lib/python3.10/site-packages (from ultralytics==8.2.102) (0.13.2)\n",
      "Requirement already satisfied: psutil in /home/areebadnan/Areeb_Python_Environments/yolo_env1/lib/python3.10/site-packages (from ultralytics==8.2.102) (6.0.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /home/areebadnan/Areeb_Python_Environments/yolo_env1/lib/python3.10/site-packages (from ultralytics==8.2.102) (6.0.1)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in /home/areebadnan/Areeb_Python_Environments/yolo_env1/lib/python3.10/site-packages (from ultralytics==8.2.102) (0.19.0+cu124)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in /home/areebadnan/Areeb_Python_Environments/yolo_env1/lib/python3.10/site-packages (from ultralytics==8.2.102) (3.9.0)\n",
      "Collecting py-cpuinfo\n",
      "  Using cached py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in /home/areebadnan/Areeb_Python_Environments/yolo_env1/lib/python3.10/site-packages (from ultralytics==8.2.102) (4.10.0.84)\n",
      "Collecting numpy<2.0.0,>=1.23.0\n",
      "  Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /home/areebadnan/Areeb_Python_Environments/yolo_env1/lib/python3.10/site-packages (from ultralytics==8.2.102) (1.14.0)\n",
      "Requirement already satisfied: torch>=1.8.0 in /home/areebadnan/Areeb_Python_Environments/yolo_env1/lib/python3.10/site-packages (from ultralytics==8.2.102) (2.4.0+cu124)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in /home/areebadnan/Areeb_Python_Environments/yolo_env1/lib/python3.10/site-packages (from ultralytics==8.2.102) (4.66.5)\n",
      "Requirement already satisfied: pandas>=1.1.4 in /home/areebadnan/Areeb_Python_Environments/yolo_env1/lib/python3.10/site-packages (from ultralytics==8.2.102) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.23.0 in /home/areebadnan/Areeb_Python_Environments/yolo_env1/lib/python3.10/site-packages (from ultralytics==8.2.102) (2.32.3)\n",
      "Requirement already satisfied: pillow>=7.1.2 in /home/areebadnan/Areeb_Python_Environments/yolo_env1/lib/python3.10/site-packages (from ultralytics==8.2.102) (10.2.0)\n",
      "Collecting ultralytics-thop>=2.0.0\n",
      "  Downloading ultralytics_thop-2.0.8-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/areebadnan/Areeb_Python_Environments/yolo_env1/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics==8.2.102) (0.12.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/areebadnan/Areeb_Python_Environments/yolo_env1/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics==8.2.102) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/areebadnan/Areeb_Python_Environments/yolo_env1/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics==8.2.102) (24.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/areebadnan/Areeb_Python_Environments/yolo_env1/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics==8.2.102) (1.2.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/areebadnan/Areeb_Python_Environments/yolo_env1/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics==8.2.102) (4.53.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/areebadnan/Areeb_Python_Environments/yolo_env1/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics==8.2.102) (2.9.0.post0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/areebadnan/Areeb_Python_Environments/yolo_env1/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics==8.2.102) (3.1.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/areebadnan/Areeb_Python_Environments/yolo_env1/lib/python3.10/site-packages (from pandas>=1.1.4->ultralytics==8.2.102) (2024.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/areebadnan/Areeb_Python_Environments/yolo_env1/lib/python3.10/site-packages (from pandas>=1.1.4->ultralytics==8.2.102) (2024.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/areebadnan/Areeb_Python_Environments/yolo_env1/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics==8.2.102) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/areebadnan/Areeb_Python_Environments/yolo_env1/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics==8.2.102) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/areebadnan/Areeb_Python_Environments/yolo_env1/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics==8.2.102) (2024.7.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/areebadnan/Areeb_Python_Environments/yolo_env1/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics==8.2.102) (3.3.2)\n",
      "Requirement already satisfied: fsspec in /home/areebadnan/Areeb_Python_Environments/yolo_env1/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics==8.2.102) (2024.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.99 in /home/areebadnan/Areeb_Python_Environments/yolo_env1/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics==8.2.102) (12.4.99)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/areebadnan/Areeb_Python_Environments/yolo_env1/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics==8.2.102) (4.12.2)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.0.99 in /home/areebadnan/Areeb_Python_Environments/yolo_env1/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics==8.2.102) (11.6.0.99)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.99 in /home/areebadnan/Areeb_Python_Environments/yolo_env1/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics==8.2.102) (12.4.99)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.0.142 in /home/areebadnan/Areeb_Python_Environments/yolo_env1/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics==8.2.102) (12.3.0.142)\n",
      "Requirement already satisfied: filelock in /home/areebadnan/Areeb_Python_Environments/yolo_env1/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics==8.2.102) (3.13.1)\n",
      "Requirement already satisfied: sympy in /home/areebadnan/Areeb_Python_Environments/yolo_env1/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics==8.2.102) (1.12)\n",
      "Requirement already satisfied: triton==3.0.0 in /home/areebadnan/Areeb_Python_Environments/yolo_env1/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics==8.2.102) (3.0.0)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.99 in /home/areebadnan/Areeb_Python_Environments/yolo_env1/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics==8.2.102) (12.4.99)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.99 in /home/areebadnan/Areeb_Python_Environments/yolo_env1/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics==8.2.102) (12.4.99)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.119 in /home/areebadnan/Areeb_Python_Environments/yolo_env1/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics==8.2.102) (10.3.5.119)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.99 in /home/areebadnan/Areeb_Python_Environments/yolo_env1/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics==8.2.102) (12.4.99)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/areebadnan/Areeb_Python_Environments/yolo_env1/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics==8.2.102) (2.20.5)\n",
      "Requirement already satisfied: networkx in /home/areebadnan/Areeb_Python_Environments/yolo_env1/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics==8.2.102) (3.2.1)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/areebadnan/Areeb_Python_Environments/yolo_env1/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics==8.2.102) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.2.65 in /home/areebadnan/Areeb_Python_Environments/yolo_env1/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics==8.2.102) (12.4.2.65)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.0.44 in /home/areebadnan/Areeb_Python_Environments/yolo_env1/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics==8.2.102) (11.2.0.44)\n",
      "Requirement already satisfied: jinja2 in /home/areebadnan/Areeb_Python_Environments/yolo_env1/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics==8.2.102) (3.1.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/areebadnan/Areeb_Python_Environments/yolo_env1/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics==8.2.102) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/areebadnan/Areeb_Python_Environments/yolo_env1/lib/python3.10/site-packages (from jinja2->torch>=1.8.0->ultralytics==8.2.102) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/areebadnan/Areeb_Python_Environments/yolo_env1/lib/python3.10/site-packages (from sympy->torch>=1.8.0->ultralytics==8.2.102) (1.3.0)\n",
      "Building wheels for collected packages: ultralytics\n",
      "  Building editable for ultralytics (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ultralytics: filename=ultralytics-8.2.102-0.editable-py3-none-any.whl size=23198 sha256=ce8b1012f1667114b4f92e6b182a8e654dcc9d7c8b7ae40123085f74a47fde0e\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-i535hf_0/wheels/1b/9a/d3/1507bbb17e474f7c313b4f6d5d17353dfa80eb78b6f4da2ad3\n",
      "Successfully built ultralytics\n",
      "Installing collected packages: py-cpuinfo, numpy, ultralytics-thop, ultralytics\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.0.1\n",
      "    Uninstalling numpy-2.0.1:\n",
      "      Successfully uninstalled numpy-2.0.1\n",
      "Successfully installed numpy-1.26.4 py-cpuinfo-9.0.0 ultralytics-8.2.102 ultralytics-thop-2.0.8\n"
     ]
    }
   ],
   "source": [
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new Ultralytics Settings v0.0.6 file âœ… \n",
      "View Ultralytics Settings with 'yolo settings' or at '/home/areebadnan/.config/Ultralytics/settings.json'\n",
      "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x384 1 person, 30.4ms\n",
      "Speed: 1.8ms preprocess, 30.4ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.1ms\n",
      "Speed: 0.8ms preprocess, 3.1ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.2ms\n",
      "Speed: 0.9ms preprocess, 3.2ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 2.8ms\n",
      "Speed: 1.0ms preprocess, 2.8ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.0ms\n",
      "Speed: 1.1ms preprocess, 3.0ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.0ms\n",
      "Speed: 0.9ms preprocess, 3.0ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.0ms\n",
      "Speed: 1.0ms preprocess, 3.0ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.2ms\n",
      "Speed: 1.0ms preprocess, 3.2ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 2.9ms\n",
      "Speed: 1.0ms preprocess, 2.9ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.0ms\n",
      "Speed: 1.0ms preprocess, 3.0ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.0ms\n",
      "Speed: 0.9ms preprocess, 3.0ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.3ms\n",
      "Speed: 1.0ms preprocess, 3.3ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.1ms\n",
      "Speed: 1.0ms preprocess, 3.1ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.0ms\n",
      "Speed: 0.8ms preprocess, 3.0ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.4ms\n",
      "Speed: 0.9ms preprocess, 3.4ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.0ms\n",
      "Speed: 1.0ms preprocess, 3.0ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 2.9ms\n",
      "Speed: 0.9ms preprocess, 2.9ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.2ms\n",
      "Speed: 0.9ms preprocess, 3.2ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 2.9ms\n",
      "Speed: 0.9ms preprocess, 2.9ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.0ms\n",
      "Speed: 0.9ms preprocess, 3.0ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.0ms\n",
      "Speed: 1.0ms preprocess, 3.0ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.3ms\n",
      "Speed: 1.0ms preprocess, 3.3ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.1ms\n",
      "Speed: 1.0ms preprocess, 3.1ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.2ms\n",
      "Speed: 1.2ms preprocess, 3.2ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.2ms\n",
      "Speed: 1.0ms preprocess, 3.2ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.1ms\n",
      "Speed: 1.0ms preprocess, 3.1ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.0ms\n",
      "Speed: 1.0ms preprocess, 3.0ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.0ms\n",
      "Speed: 1.0ms preprocess, 3.0ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.3ms\n",
      "Speed: 0.9ms preprocess, 3.3ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.4ms\n",
      "Speed: 0.9ms preprocess, 3.4ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.1ms\n",
      "Speed: 0.9ms preprocess, 3.1ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.1ms\n",
      "Speed: 1.0ms preprocess, 3.1ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.3ms\n",
      "Speed: 1.0ms preprocess, 3.3ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.1ms\n",
      "Speed: 0.9ms preprocess, 3.1ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.0ms\n",
      "Speed: 0.9ms preprocess, 3.0ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.2ms\n",
      "Speed: 0.9ms preprocess, 3.2ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.0ms\n",
      "Speed: 0.9ms preprocess, 3.0ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.0ms\n",
      "Speed: 1.1ms preprocess, 3.0ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.3ms\n",
      "Speed: 0.9ms preprocess, 3.3ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.0ms\n",
      "Speed: 1.1ms preprocess, 3.0ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 2.9ms\n",
      "Speed: 1.0ms preprocess, 2.9ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.1ms\n",
      "Speed: 0.9ms preprocess, 3.1ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.3ms\n",
      "Speed: 1.1ms preprocess, 3.3ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.2ms\n",
      "Speed: 1.0ms preprocess, 3.2ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.1ms\n",
      "Speed: 1.0ms preprocess, 3.1ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.0ms\n",
      "Speed: 1.0ms preprocess, 3.0ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 2.8ms\n",
      "Speed: 1.0ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.1ms\n",
      "Speed: 0.9ms preprocess, 3.1ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.1ms\n",
      "Speed: 1.0ms preprocess, 3.1ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.1ms\n",
      "Speed: 1.0ms preprocess, 3.1ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.0ms\n",
      "Speed: 1.0ms preprocess, 3.0ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 2.9ms\n",
      "Speed: 1.0ms preprocess, 2.9ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.1ms\n",
      "Speed: 0.9ms preprocess, 3.1ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 2.9ms\n",
      "Speed: 0.9ms preprocess, 2.9ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.0ms\n",
      "Speed: 0.9ms preprocess, 3.0ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 2.8ms\n",
      "Speed: 1.0ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.1ms\n",
      "Speed: 0.9ms preprocess, 3.1ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.1ms\n",
      "Speed: 1.0ms preprocess, 3.1ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.2ms\n",
      "Speed: 1.0ms preprocess, 3.2ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.0ms\n",
      "Speed: 0.9ms preprocess, 3.0ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.0ms\n",
      "Speed: 1.0ms preprocess, 3.0ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.1ms\n",
      "Speed: 1.0ms preprocess, 3.1ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 2.9ms\n",
      "Speed: 1.0ms preprocess, 2.9ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.1ms\n",
      "Speed: 1.0ms preprocess, 3.1ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.0ms\n",
      "Speed: 0.8ms preprocess, 3.0ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.3ms\n",
      "Speed: 0.9ms preprocess, 3.3ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.2ms\n",
      "Speed: 0.9ms preprocess, 3.2ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.0ms\n",
      "Speed: 1.0ms preprocess, 3.0ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.7ms\n",
      "Speed: 1.0ms preprocess, 3.7ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.1ms\n",
      "Speed: 1.4ms preprocess, 3.1ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 2.9ms\n",
      "Speed: 1.1ms preprocess, 2.9ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.2ms\n",
      "Speed: 1.1ms preprocess, 3.2ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.4ms\n",
      "Speed: 1.0ms preprocess, 3.4ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.2ms\n",
      "Speed: 0.8ms preprocess, 3.2ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.1ms\n",
      "Speed: 1.0ms preprocess, 3.1ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.1ms\n",
      "Speed: 1.0ms preprocess, 3.1ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.1ms\n",
      "Speed: 0.9ms preprocess, 3.1ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.0ms\n",
      "Speed: 0.9ms preprocess, 3.0ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 2.9ms\n",
      "Speed: 0.9ms preprocess, 2.9ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.0ms\n",
      "Speed: 0.9ms preprocess, 3.0ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.3ms\n",
      "Speed: 1.2ms preprocess, 3.3ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.3ms\n",
      "Speed: 0.9ms preprocess, 3.3ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.3ms\n",
      "Speed: 1.0ms preprocess, 3.3ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.4ms\n",
      "Speed: 0.9ms preprocess, 3.4ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.4ms\n",
      "Speed: 1.0ms preprocess, 3.4ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.3ms\n",
      "Speed: 0.9ms preprocess, 3.3ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.0ms\n",
      "Speed: 0.9ms preprocess, 3.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 4.2ms\n",
      "Speed: 1.3ms preprocess, 4.2ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 2.9ms\n",
      "Speed: 0.9ms preprocess, 2.9ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.2ms\n",
      "Speed: 0.9ms preprocess, 3.2ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.2ms\n",
      "Speed: 1.4ms preprocess, 3.2ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.2ms\n",
      "Speed: 0.9ms preprocess, 3.2ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.3ms\n",
      "Speed: 1.0ms preprocess, 3.3ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.4ms\n",
      "Speed: 0.9ms preprocess, 3.4ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.7ms\n",
      "Speed: 1.4ms preprocess, 3.7ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.1ms\n",
      "Speed: 1.0ms preprocess, 3.1ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.0ms\n",
      "Speed: 0.9ms preprocess, 3.0ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 2.9ms\n",
      "Speed: 1.0ms preprocess, 2.9ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 2.9ms\n",
      "Speed: 1.0ms preprocess, 2.9ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 4.6ms\n",
      "Speed: 1.0ms preprocess, 4.6ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 5.8ms\n",
      "Speed: 1.7ms preprocess, 5.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 5.3ms\n",
      "Speed: 1.6ms preprocess, 5.3ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.5ms\n",
      "Speed: 0.9ms preprocess, 3.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.6ms\n",
      "Speed: 1.0ms preprocess, 3.6ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 2.9ms\n",
      "Speed: 0.9ms preprocess, 2.9ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 2.9ms\n",
      "Speed: 1.0ms preprocess, 2.9ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.0ms\n",
      "Speed: 1.0ms preprocess, 3.0ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 4.7ms\n",
      "Speed: 0.9ms preprocess, 4.7ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.3ms\n",
      "Speed: 0.9ms preprocess, 3.3ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.9ms\n",
      "Speed: 1.0ms preprocess, 3.9ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.3ms\n",
      "Speed: 0.9ms preprocess, 3.3ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.0ms\n",
      "Speed: 0.9ms preprocess, 3.0ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.1ms\n",
      "Speed: 0.9ms preprocess, 3.1ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.1ms\n",
      "Speed: 0.9ms preprocess, 3.1ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.0ms\n",
      "Speed: 0.8ms preprocess, 3.0ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.0ms\n",
      "Speed: 1.1ms preprocess, 3.0ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.2ms\n",
      "Speed: 0.9ms preprocess, 3.2ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.1ms\n",
      "Speed: 0.9ms preprocess, 3.1ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.1ms\n",
      "Speed: 1.1ms preprocess, 3.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.5ms\n",
      "Speed: 1.2ms preprocess, 3.5ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 2.9ms\n",
      "Speed: 1.2ms preprocess, 2.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 2.9ms\n",
      "Speed: 0.9ms preprocess, 2.9ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.1ms\n",
      "Speed: 1.2ms preprocess, 3.1ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.0ms\n",
      "Speed: 1.0ms preprocess, 3.0ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.4ms\n",
      "Speed: 0.8ms preprocess, 3.4ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 2.9ms\n",
      "Speed: 1.0ms preprocess, 2.9ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.2ms\n",
      "Speed: 1.0ms preprocess, 3.2ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.4ms\n",
      "Speed: 0.9ms preprocess, 3.4ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 3.0ms\n",
      "Speed: 0.9ms preprocess, 3.0ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Output video saved at /home/areebadnan/Areeb_code/work/Visua_Data/videos/pose_video2_output.mp4\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load the YOLOv8 model\n",
    "model = YOLO(\"yolov8n-pose.pt\") \n",
    "\n",
    "# Open the video file\n",
    "video_path = \"/home/areebadnan/Areeb_code/work/Visua_Data/videos/614a1318181b3ef94faeb847.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Get video properties (width, height, frames per second)\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "# Define the codec and create a VideoWriter object to save the output video\n",
    "output_path = \"/home/areebadnan/Areeb_code/work/Visua_Data/videos/pose_video2_output.mp4\"\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")  # Codec for mp4 format\n",
    "out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "# Loop through the video frames\n",
    "while cap.isOpened():\n",
    "    # Read a frame from the video\n",
    "    success, frame = cap.read()\n",
    "\n",
    "    if success:\n",
    "        # Run YOLOv8 inference on the frame\n",
    "        results = model(frame)\n",
    "\n",
    "        # Visualize the results on the frame\n",
    "        annotated_frame = results[0].plot()\n",
    "\n",
    "        # Write the annotated frame to the output video\n",
    "        out.write(annotated_frame)\n",
    "\n",
    "        # Display the annotated frame (optional)\n",
    "        #cv2.imshow(\"YOLOv8 Inference\", annotated_frame)\n",
    "\n",
    "        # Break the loop if 'q' is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "    else:\n",
    "        # Break the loop if the end of the video is reached\n",
    "        break\n",
    "\n",
    "# Release the video capture and writer objects, and close the display window\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(f\"Output video saved at {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keypoint label conversion complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "\n",
    "# Provide full paths for labels and images folders\n",
    "labels_dir = \"/home/areebadnan/Areeb_code/work/Atheritia/Notebooks/YOLOv8_Architecture_Modification/YOLOv8_Architecture_Modification/Pose_Head_Addition/3_logos_dataset/train/labels\"  # Full path to the folder containing .txt label files\n",
    "images_dir = \"/home/areebadnan/Areeb_code/work/Atheritia/Notebooks/YOLOv8_Architecture_Modification/YOLOv8_Architecture_Modification/Pose_Head_Addition/3_logos_dataset/train/images\"  # Full path to the folder containing .jpg image files\n",
    "\n",
    "# Find all .txt label files in the labels directory\n",
    "labels = glob(f\"{labels_dir}/*.txt\")\n",
    "\n",
    "# Define the root directory for saving keypoint labels\n",
    "output_dir = Path(labels_dir).parent / \"labels_kp\"  # You can change the output folder if needed\n",
    "output_dir.mkdir(exist_ok=True)  # Create output directory if it doesn't exist\n",
    "\n",
    "for label in labels:\n",
    "    label = Path(label)\n",
    "    \n",
    "    # Find the corresponding image in the images folder (replace .txt with .jpg)\n",
    "    img_path = Path(images_dir) / label.with_suffix('.jpg').name\n",
    "\n",
    "    # If image does not exist, skip the file\n",
    "    if not img_path.exists():\n",
    "        print(f\"Image not found for label {label.name}\")\n",
    "        continue\n",
    "\n",
    "    boxes = []\n",
    "    points = []\n",
    "    classes = []\n",
    "    \n",
    "    # Define the save path for keypoint labels\n",
    "    save_pth = output_dir / label.name\n",
    "    save_pth.parent.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Read the label file and process the content\n",
    "    with open(label) as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            splits = line.rstrip().split(\" \")\n",
    "            cls_id = int(splits[0])\n",
    "            box = splits[1:]\n",
    "            \n",
    "            # If the box data is empty, skip this line\n",
    "            if not box:\n",
    "                with open(save_pth, \"w\") as f:\n",
    "                    pass\n",
    "                continue\n",
    "\n",
    "            # Convert the box information into floats and treat the center as a keypoint\n",
    "            box = [float(pt) for pt in box]\n",
    "            point = (box[0], box[1])\n",
    "            points.append(point)\n",
    "            boxes.append(box)\n",
    "            classes.append(cls_id)\n",
    "\n",
    "    # Write the new keypoint label file\n",
    "    with open(save_pth, \"w\") as f:\n",
    "        for point, box, cls_id in zip(points, boxes, classes):\n",
    "            f.writelines(f\"{cls_id} {box[0]} {box[1]} {box[2]} {box[3]} {point[0]} {point[1]} 1 \\n\")\n",
    "\n",
    "print(\"Keypoint label conversion complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo_env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
